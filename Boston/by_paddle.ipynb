{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T03:50:03.418000Z",
     "start_time": "2020-01-01T03:50:01.792000Z"
    }
   },
   "outputs": [],
   "source": [
    "#加载飞桨、Numpy和相关类库\n",
    "import paddle                            # 导入paddle模块\n",
    "import paddle.fluid as fluid\n",
    "import paddle.fluid.dygraph as dygraph\n",
    "from paddle.fluid.dygraph import FC\n",
    "import numpy as np\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 数据处理（Python实现）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T03:50:03.427000Z",
     "start_time": "2020-01-01T03:50:03.419000Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    # 从文件导入数据\n",
    "    datafile = './housing.data'\n",
    "    data = np.fromfile(datafile, sep=' ')\n",
    "\n",
    "    # 每条数据包括14项，其中前面13项是影响因素，第14项是相应的房屋价格中位数\n",
    "    feature_names = [ 'CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', \\\n",
    "                      'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV' ]\n",
    "    feature_num = len(feature_names)\n",
    "\n",
    "    # 将原始数据进行Reshape，变成[N, 14]这样的形状\n",
    "    data = data.reshape([data.shape[0] // feature_num, feature_num])\n",
    "\n",
    "    # 将原数据集拆分成训练集和测试集\n",
    "    # 这里使用80%的数据做训练，20%的数据做测试\n",
    "    # 测试集和训练集必须是没有交集的\n",
    "    ratio = 0.8\n",
    "    offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "\n",
    "    # 计算train数据集的最大值，最小值，平均值\n",
    "    maximums, minimums, avgs = training_data.max(axis=0), training_data.min(axis=0), \\\n",
    "                                 training_data.sum(axis=0) / training_data.shape[0]\n",
    "    \n",
    "    # 记录数据的归一化参数，在预测时对数据做归一化\n",
    "    global max_values\n",
    "    global min_values\n",
    "    global avg_values\n",
    "    max_values = maximums\n",
    "    min_values = minimums\n",
    "    avg_values = avgs\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(feature_num):\n",
    "        #print(maximums[i], minimums[i], avgs[i])\n",
    "        data[:, i] = (data[:, i] - avgs[i]) / (maximums[i] - minimums[i])\n",
    "\n",
    "    # 训练集和测试集的划分比例\n",
    "    #ratio = 0.8\n",
    "    #offset = int(data.shape[0] * ratio)\n",
    "    training_data = data[:offset]\n",
    "    test_data = data[offset:]\n",
    "    return training_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 模型设计（建议Python实现）\n",
    "定义线性回归的网络结构，飞桨建议通过创建Python类的方式完成模型网络的定义。\n",
    "\n",
    "在类的初始化函数中定义每一层网络的实现函数，这里我们定义了一层全连接层FC，模型结构和1-2 节模型保持一致。  \n",
    "定义forward函数构建神经网络结构，实现前向计算过程，并返回预测结果，本例中返回的是房价预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "用的是 Python 2 的话，super() 替换为 suepr(Class, self) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T03:50:03.466000Z",
     "start_time": "2020-01-01T03:50:03.430000Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# fluid.dygraph.Layer: 基于OOD实现的动态图Layer，包含该Layer的参数、前序运行的结构等信息。\n",
    "class Regressor(fluid.dygraph.Layer):\n",
    "    def __init__(self, name_scope):\n",
    "        super(Regressor, self).__init__(name_scope)\n",
    "        name_scope = self.full_name()\n",
    "        print(name_scope)\n",
    "        # 定义一层全连接层，输出维度是1，激活函数为None，即不使用激活函数\n",
    "        self.fc = FC(name_scope, size=1, act=None)\n",
    "    \n",
    "    # 网络的前向计算函数\n",
    "    def forward(self, inputs):\n",
    "        x = self.fc(inputs)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 训练配置\n",
    "训练配置包括：\n",
    "\n",
    "声明定义好的模型。  \n",
    "加载训练数据和测试数据。  \n",
    "设置优化算法和学习率，本次实验优化算法使用随机梯度下降SGD，学习率使用 0.01。   \n",
    "说明：  \n",
    "在之前基于Python实现神经网络模型的案例中，我们为实现梯度下降编写了大量代码，而使用飞桨框架可以大大简化这个过程。\n",
    "\n",
    "### with fluid.dygraph.guard()创建了飞桨动态图的工作环境，在该环境下完成模型声明、数据转换、以及模型训练等操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T03:50:03.489000Z",
     "start_time": "2020-01-01T03:50:03.468000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regressor/Regressor_0\n"
     ]
    }
   ],
   "source": [
    "# 定义飞桨动态图的工作环境\n",
    "# place (fluid.CPUPlace|fluid.CUDAPlace, 可选) – 动态图执行的设备，可以选择cpu，gpu，\n",
    "# 如果用户未制定，则根据用户paddle编译的方式来选择运行的设备，如果编译的cpu版本，则在cpu上运行，\n",
    "# 如果是编译的gpu版本，则在gpu上运行。默认值：None。\n",
    "with fluid.dygraph.guard():\n",
    "    # 声明定义好的线性回归模型\n",
    "    model = Regressor(\"Regressor\")\n",
    "    # 开启模型训练模式\n",
    "    model.train()\n",
    "    # 加载数据\n",
    "    training_data, test_data = load_data()\n",
    "    # 定义优化算法，这里使用随机梯度下降-SGD\n",
    "    # 学习率设置为0.01\n",
    "    opt = fluid.optimizer.SGD(learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 训练过程\n",
    "完成训练配置后即可启动训练过程。训练采用二层循环嵌套方式：\n",
    "\n",
    "**内层循环**负责整个数据集的一次遍历，遍历数据集采用分批次（batch）方式，假设数据集样本数量为1000，一个批次有10个样本，则遍历一次数据集的批次数量是1000/10=100，即内层循环需要循环100次；  \n",
    "**外层循环**定义遍历数据集的次数，本次训练中外层循环10次，通过参数EPOCH_NUM设置；  \n",
    "\n",
    "说明:  \n",
    "batch大小的选择会影响训练效果，batch过大会增大内存消耗，过小则每个batch的样本数据没有统计意义。本次训练数据集较小，我们设置batch为10。\n",
    "\n",
    "在每次内层循环都需要进行前向计算、损失函数计算和梯度反向传播三个步骤，计算的过程与Python编写的模型完全一致；\n",
    "\n",
    "前向计算即将一个批次的样本数据灌入网络中，计算输出结果。  \n",
    "以前向计算结果和真实房价作为输入，通过损失函数square_error_cost计算出损失函数值（Loss）。  \n",
    "执行梯度反向传播backward函数，即从后到前逐层计算每一层的梯度，并根据设置的优化算法更新参数（opt.minimize）。  \n",
    "这个实现过程令人惊喜，前向计算、计算损失和反向传播梯度，每个操作居然只有1-2行代码即可实现！我们再也不用一点点的实现模型训练的细节，这就是使用飞桨框架的威力！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T03:50:03.694000Z",
     "start_time": "2020-01-01T03:50:03.490000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, iter: 0, loss is: [0.39176014]\n",
      "epoch: 1, iter: 0, loss is: [0.35759827]\n",
      "epoch: 2, iter: 0, loss is: [0.37494797]\n",
      "epoch: 3, iter: 0, loss is: [0.33708817]\n",
      "epoch: 4, iter: 0, loss is: [0.3066762]\n",
      "epoch: 5, iter: 0, loss is: [0.29184678]\n",
      "epoch: 6, iter: 0, loss is: [0.30939215]\n",
      "epoch: 7, iter: 0, loss is: [0.29835087]\n",
      "epoch: 8, iter: 0, loss is: [0.26178697]\n",
      "epoch: 9, iter: 0, loss is: [0.27210417]\n",
      "epoch: 10, iter: 0, loss is: [0.2922308]\n",
      "epoch: 11, iter: 0, loss is: [0.24840203]\n",
      "epoch: 12, iter: 0, loss is: [0.28776324]\n",
      "epoch: 13, iter: 0, loss is: [0.2649998]\n",
      "epoch: 14, iter: 0, loss is: [0.24687095]\n",
      "epoch: 15, iter: 0, loss is: [0.2222822]\n",
      "epoch: 16, iter: 0, loss is: [0.25335276]\n",
      "epoch: 17, iter: 0, loss is: [0.24512134]\n",
      "epoch: 18, iter: 0, loss is: [0.22216968]\n",
      "epoch: 19, iter: 0, loss is: [0.21300644]\n",
      "epoch: 20, iter: 0, loss is: [0.20396289]\n",
      "epoch: 21, iter: 0, loss is: [0.21724041]\n",
      "epoch: 22, iter: 0, loss is: [0.2019036]\n",
      "epoch: 23, iter: 0, loss is: [0.18444832]\n",
      "epoch: 24, iter: 0, loss is: [0.197561]\n",
      "epoch: 25, iter: 0, loss is: [0.1897604]\n",
      "epoch: 26, iter: 0, loss is: [0.17664996]\n",
      "epoch: 27, iter: 0, loss is: [0.1709741]\n",
      "epoch: 28, iter: 0, loss is: [0.16076687]\n",
      "epoch: 29, iter: 0, loss is: [0.1627573]\n",
      "epoch: 30, iter: 0, loss is: [0.16005051]\n",
      "epoch: 31, iter: 0, loss is: [0.16787034]\n",
      "epoch: 32, iter: 0, loss is: [0.1700897]\n",
      "epoch: 33, iter: 0, loss is: [0.1436063]\n",
      "epoch: 34, iter: 0, loss is: [0.1588128]\n",
      "epoch: 35, iter: 0, loss is: [0.1388847]\n",
      "epoch: 36, iter: 0, loss is: [0.14248963]\n",
      "epoch: 37, iter: 0, loss is: [0.14236611]\n",
      "epoch: 38, iter: 0, loss is: [0.13750193]\n",
      "epoch: 39, iter: 0, loss is: [0.12944789]\n",
      "epoch: 40, iter: 0, loss is: [0.13750882]\n",
      "epoch: 41, iter: 0, loss is: [0.12584743]\n",
      "epoch: 42, iter: 0, loss is: [0.1340062]\n",
      "epoch: 43, iter: 0, loss is: [0.12965757]\n",
      "epoch: 44, iter: 0, loss is: [0.12995622]\n",
      "epoch: 45, iter: 0, loss is: [0.13188541]\n",
      "epoch: 46, iter: 0, loss is: [0.12819517]\n",
      "epoch: 47, iter: 0, loss is: [0.13400817]\n",
      "epoch: 48, iter: 0, loss is: [0.12804785]\n",
      "epoch: 49, iter: 0, loss is: [0.11101698]\n"
     ]
    }
   ],
   "source": [
    "with dygraph.guard():\n",
    "    EPOCH_NUM = 50   # 设置外层循环次数\n",
    "    BATCH_SIZE = 200  # 设置batch大小\n",
    "    \n",
    "    # 定义外层循环\n",
    "    for epoch_id in range(EPOCH_NUM):\n",
    "        # 在每轮迭代开始之前，将训练数据的顺序随机的打乱\n",
    "        np.random.shuffle(training_data)\n",
    "        # 将训练数据进行拆分，每个batch包含10条数据\n",
    "        mini_batches = [training_data[k:k+BATCH_SIZE] for k in range(0, len(training_data), BATCH_SIZE)]\n",
    "        # 定义内层循环\n",
    "        for iter_id, mini_batch in enumerate(mini_batches):\n",
    "            x = np.array(mini_batch[:, :-1]).astype('float32') # 获得当前批次训练数据\n",
    "            y = np.array(mini_batch[:, -1:]).astype('float32') # 获得当前批次训练标签（真实房价）\n",
    "            # 将numpy数据转为飞桨动态图variable形式\n",
    "            house_features = dygraph.to_variable(x)\n",
    "            prices = dygraph.to_variable(y)\n",
    "            \n",
    "            # 前向计算\n",
    "            predicts = model(house_features)\n",
    "            \n",
    "            # 计算损失\n",
    "            loss = fluid.layers.square_error_cost(predicts, label=prices)\n",
    "            avg_loss = fluid.layers.mean(fluid.layers.sqrt(loss))\n",
    "            if iter_id%20==0:\n",
    "                print(\"epoch: {}, iter: {}, loss is: {}\".format(epoch_id, iter_id, avg_loss.numpy()))\n",
    "            \n",
    "            # 反向传播\n",
    "            avg_loss.backward()\n",
    "            # 最小化loss,更新参数\n",
    "            # opt 优化方法 SGD最小化\n",
    "            opt.minimize(avg_loss)\n",
    "            # 清除梯度\n",
    "            model.clear_gradients()\n",
    "    # 保存模型\n",
    "    fluid.save_dygraph(model.state_dict(), 'LR_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 保存并测试模型\n",
    "在完成两层循环的训练过程后，将模型当前的参数（model.state_dict()）保存到文件中（通过参数指定保存的文件名 LR_model），以备预测或校验的程序调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T03:50:04.003000Z",
     "start_time": "2020-01-01T03:50:03.697000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型保存成功，模型参数保存在LR_model中\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'y_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f6d89218c6ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m fluid.io.save_inference_model(\"./boston_model1\",   #保存推理model的路径\n\u001b[0;32m     11\u001b[0m                                   \u001b[1;33m[\u001b[0m\u001b[1;34m'x'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m            \u001b[1;31m#推理（inference）需要 feed 的数据\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                                   \u001b[1;33m[\u001b[0m\u001b[0my_predict\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m      \u001b[1;31m#保存推理（inference）结果的 Variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                                   exe)              #exe 保存 inference model\n\u001b[0;32m     14\u001b[0m \u001b[0mfluid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"./boston_model1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_predict' is not defined"
     ]
    }
   ],
   "source": [
    "# 定义飞桨动态图工作环境\n",
    "with fluid.dygraph.guard():\n",
    "    # 保存模型参数，文件名为LR_model\n",
    "    # paddle.fluid.dygraph.save_dygraph(state_dict, model_path)\n",
    "    # state_dict 是通过 Layer 的 state_dict() 方法得到的。\n",
    "    fluid.save_dygraph(model.state_dict(), 'LR_model')\n",
    "    print(\"模型保存成功，模型参数保存在LR_model中\")\n",
    "    \n",
    "#保存训练参数到指定路径中，构建一个专门用预测的program\n",
    "fluid.io.save_inference_model(\"./boston_model1\",   #保存推理model的路径\n",
    "                                  ['x'],            #推理（inference）需要 feed 的数据\n",
    "                                  [y_predict],      #保存推理（inference）结果的 Variables\n",
    "                                  exe)              #exe 保存 inference model\n",
    "fluid.io.save_params(exe,\"./boston_model1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回顾下基于飞桨实现的房价预测模型，实现的效果与之前基于Python实现的模型没有区别，但两者的实现成本有天壤之别。飞桨的愿景是用户只需要了解模型的逻辑概念，不需要关心实现细节，就能搭建强大的模型。\n",
    "\n",
    "下面我们选择一条数据样本，测试下模型的预测效果。 测试过程和在应用场景中使用模型的过程是一致的，可分成三个主要步骤。\n",
    "\n",
    "首先，**配置模型预测的机器资源**，本案例默认使用本机，所以无需写代码指定。    \n",
    "其次，**将训练好的模型参数加载到模型实例中**，由两个语句完成，第一句是从文件中读取模型参数，第二句是将参数内容加载到模型。加载完毕后，需要将模型的状态调整为“校验”（evalueation）。这是因为训练状态的模型需要同时支持前向计算和反向传导梯度，模型的实现较为臃肿。而校验/预测状态的模型只需要支持前向计算，模型的实现更加简单，性能更好。  \n",
    "最后，**将待预测的样本特征输入到模型中，打印输出的预测结果**。比较“模型预测值”和“真实房价”可见，模型预测的效果与真实房价接近。\n",
    "load_one_example函数实现了从数据集中抽一条样本作为测试样本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T03:50:04.004000Z",
     "start_time": "2020-01-01T03:50:01.806Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_one_example(data_dir):\n",
    "    f = open(data_dir, 'r')\n",
    "    datas = f.readlines()\n",
    "    # 选择倒数第10条数据用于测试\n",
    "    tmp = datas[-10]\n",
    "    tmp = tmp.strip().split()\n",
    "    one_data = [float(v) for v in tmp]\n",
    "\n",
    "    # 对数据进行归一化处理\n",
    "    for i in range(len(one_data)-1):\n",
    "        one_data[i] = (one_data[i] - avg_values[i]) / (max_values[i] - min_values[i])\n",
    "\n",
    "    data = np.reshape(np.array(one_data[:-1]), [1, -1]).astype(np.float32)\n",
    "    label = one_data[-1]\n",
    "    return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-01-01T03:50:04.005000Z",
     "start_time": "2020-01-01T03:50:01.808Z"
    }
   },
   "outputs": [],
   "source": [
    "with dygraph.guard():\n",
    "    # 参数为保存模型参数的文件地址\n",
    "    # 该接口尝试从磁盘中加载参数或优化器的 dict\n",
    "    model_dict, _ = fluid.load_dygraph('LR_model')\n",
    "    model.load_dict(model_dict)\n",
    "    # 在预测的模式下，DyGraph将只会执行前向的预测网络，而不会进行自动求导并执行反向网络\n",
    "    # 当我们需要在训练的过程中进行预测时需要使用YourModel.eval()切换到预测模式，\n",
    "    # 并且在预测完成后使用YourModel.train()切换回训练模式继续训练\n",
    "    model.eval()\n",
    "\n",
    "    # 参数为数据集的文件地址\n",
    "    test_data, label = load_one_example('./housing.data')\n",
    "    # 将数据转为动态图的variable格式\n",
    "    # value (ndarray) 需要转换的numpy.ndarray对象\n",
    "    test_data = dygraph.to_variable(test_data)\n",
    "    results = model(test_data)\n",
    "\n",
    "    # 对结果做反归一化处理\n",
    "    results = results * (max_values[-1] - min_values[-1]) + avg_values[-1]\n",
    "    print(\"Inference result is {}, the corresponding label is {}\".format(results.numpy(), label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
